{
  "attn_implementation": "eager",
  "bf16": false,
  "bnb_4bit_compute_dtype": "auto",
  "bnb_4bit_quant_type": "nf4",
  "bnb_4bit_use_double_quant": true,
  "device_map": "",
  "do_eval": true,
  "do_test": true,
  "eval_file": "FinQA/dataset/dev.json",
  "eval_steps": 200,
  "eval_strategy": "steps",
  "fp16": true,
  "generation_do_sample": false,
  "generation_max_new_tokens": 128,
  "generation_num_beams": 1,
  "generation_temperature": 1.0,
  "generation_top_p": 1.0,
  "gradient_accumulation_steps": 4,
  "gradient_checkpointing": true,
  "input_mode": "all",
  "learning_rate": 2e-05,
  "load_best_model_at_end": true,
  "logging_steps": 10,
  "lora_alpha": 32,
  "lora_bias": "none",
  "lora_dropout": 0.05,
  "lora_modules_to_save": "",
  "lora_r": 16,
  "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
  "lr_scheduler_type": "cosine",
  "max_eval_samples": 0,
  "max_seq_length": 1024,
  "max_test_samples": 0,
  "max_train_samples": 0,
  "model_name": "google/gemma-3-4b-it",
  "noisy_context_seed": 0,
  "noisy_table_distractors": 1,
  "noisy_text_distractors": 3,
  "num_train_epochs": 1.0,
  "optim": "paged_adamw_8bit",
  "output_folder": "gemma3-4b-qlora-input_all-output_numerical",
  "peft": "qlora",
  "per_device_eval_batch_size": 2,
  "per_device_train_batch_size": 1,
  "resolved_output_dir": "__output__/gemma3-4b-qlora-input_all-output_numerical",
  "resume_from_checkpoint": null,
  "save_steps": 200,
  "save_strategy": "steps",
  "save_total_limit": 1,
  "seed": 42,
  "target_field": "numerical",
  "task_instruction": "",
  "test_file": "FinQA/dataset/test.json",
  "train_file": "FinQA/dataset/train.json",
  "wandb_group": "",
  "wandb_id": "",
  "wandb_mode": "online",
  "wandb_project": "finqa-gemma",
  "wandb_resume": "never",
  "wandb_run_name": "Gemma3-4B QLoRA numerical | input=all | 1ep",
  "wandb_tags": "",
  "warmup_ratio": 0.03,
  "weight_decay": 0.01
}